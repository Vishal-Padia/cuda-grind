#include <cuda_fp16.h>

#include "cuda_gemm.hpp"
// #include "cuda_gemm_utils.cuh"
#include "cuda_gemm_utils.hpp"

template <typename T, size_t BLOCK_TILE_SIZE_X, size_t BLOCK_TILE_SIZE_Y, size_t BLOCK_TILE_SIZE_K, size_t NUM_THREADS, size_t BLOCK_TILE_SKEW_SIZE_X = 0U, size_t BLOCK_TILE_SKEW_SIZE_Y = 0U, typename VECTOR_TYPE = int4>
__device__ void load_data_from_global_memory_to_shared_memory_transposed_vectorized(
    T const* A, size_t lda, T const* B, size_t ldb,
    T A_thread_block_tile_transposed[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_Y + BLOCK_TILE_SKEW_SIZE_Y],
    T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X + BLOCK_TILE_SKEW_SIZE_X],
    size_t thread_block_tile_idx, size_t thread_linear_idx, size_t m, size_t n, size_t k
)
{
    constexpr size_t NUM_VECTOR_UNITS{sizeof(VECTOR_TYPE) / sizeof(T)};
    static_assert(sizeof(VECTOR_TYPE) % sizeof(T) == 0U);

    static_assert(BLOCK_TILE_SIZE_K % NUM_VECTOR_UNITS == 0U);
    static_assert(BLOCK_TILE_SIZE_X % NUM_VECTOR_UNITS == 0U);

    constexpr size_t VECTORIZED_BLOCK_TILE_SIZE_X{BLOCK_TILE_SIZE_X / NUM_VECTOR_UNITS};
    static_assert(BLOCK_TILE_SIZE_X % NUM_VECTOR_UNITS == 0U);

    static_assert((BLOCK_TILE_SIZE_Y) * sizeof(T) % sizeof(VECTOR_TYPE) == 0U);
    static_assert((BLOCK_TILE_SIZE_X) * sizeof(T) % sizeof(VECTOR_TYPE) == 0U);
    static_assert((BLOCK_TILE_SIZE_Y + BLOCK_TILE_SKEW_SIZE_Y) * sizeof(T) % sizeof(VECTOR_TYPE) == 0U);
    static_assert((BLOCK_TILE_SIZE_X + BLOCK_TILE_SKEW_SIZE_X) * sizeof(T) % sizeof(VECTOR_TYPE) == 0U);

    // load data from A on DRAM
    #pragma unroll
    for (size_t load_idx{0U}; load_idx < (BLOCK_TILE_SIZE_Y * VECTORIZED_BLOCK_TILE_SIZE_K + NUM_THREADS - 1U) / NUM_THREADS; ++load_idx)
    {
        size_t const A_thread_block_tile_row_idx{(thread_linear_idx + load_idx * NUM_THREADS) / VECTORIZED_BLOCK_TILE_SIZE_K};
        size_t const A_thread_block_tile_col_idx{(thread_linear_idx + load_idx * NUM_THREADS) / VECTORIZED_BLOCK_TILE_SIZE_K * NUM_VECTOR_UNITS};

        size_t const A_row_idx{blockIdx.y * BLOCK_TILE_SIZE_Y + A_thread_block_tile_row_idx};
        size_t const A_col_idx{thread_block_tile_idx * BLOCK_TILE_SIZE_K + A_thread_block_tile_col_idx};

        // boundary check
        int4 A_row_vector_vals{0, 0, 0, 0};
        if (A_row_idx < m && A_col_idx < k)
        {
            A_row_vector_vals = *reinterpret_cast<int4 const*>(&A[A_row_idx * lda + A_col_idx]);
        }
        if (A_col_idx + NUM_VECTOR_UNITS > k)
        {
            // get invalid elements
            size_t const num_invalid_elements{A_col_idx + NUM_VECTOR_UNITS - k};
            // mask invalid elem
            T* const A_row_vector_vals_ptr{reinterpret_cast<T*>(&A_row_vector_vals)};
            for (size_t i{0U}; i < num_invalid_elements; ++i)
            {
                A_row_vector_vals_ptr[NUM_VECTOR_UNITS - 1U - i] = static_cast<T>(0);
            }
        }

        if (A_thread_block_tile_row_idx < BLOCK_TILE_SIZE_Y && A_thread_block_tile_col_idx < BLOCK_TILE_SIZE_K)
        {
            for (size_t i{0U}; i < NUM_VECTOR_UNITS; ++i)
            {
                A_thread_block_tile_transposed[A_thread_block_tile_col_idx + i][A_thread_block_tile_row_idx] = reinterpret_cast<T const*>(&A_row_vector_vals)[i];
            }
        }
    }

    // load data from B to DRAM
    #pragma unroll
    for (size_t load_idx{0U}; load_idx < (BLOCK_TILE_SIZE_K * VECTORIZED_BLOCK_TILE_SIZE_X + NUM_THREADS - 1U) / NUM_THREADS; ++load_idx)
    {
        size_t const B_thread_block_tile_row_idx{(thread_linear_idx + load_idx * NUM_THREADS) / VECTORIZED_BLOCK_TILE_SIZE_X};
        size_t const B_thread_block_tile_col_idx{(thread_linear_idx + load_idx * NUM_THREADS) / VECTORIZED_THREAD_TILE_SIZE_X + NUM_VECTOR_UNITS};

        size_t const B_row_idx{thread_block_tile_idx * BLOCK_TILE_SIZE_K + B_thread_block_tile_row_idx};
        size_t const B_col_idx{blockIdx.x * BLOCK_TILE_SIZE_K + B_thread_block_tile_col_idx};

        // boundary checks
        int4 B_row_vector_vals{0, 0, 0, 0};
        if (B_row_idx < k && B_col_idx < n)
        {
            B_row_vector_vals = *reinterpret_cast<int4 const*>(&B[B_row_idx * ldb + B_col_idx]);
        }

        if (B_col_idx + NUM_VECTOR_UNITS > n)
        {
            // invalid elem
            size_t const num_invalid_elements{B_col_idx + NUM_VECTOR_UNITS - n};
            // mask out invalid elem
            T* const B_row_vector_vals_ptr{reinterpret_cast<T*>(&B_row_vector_vals)};

            for (size_t i{0U}; i < num_invalid_elements; ++i)
            {
                B_row_vector_vals_ptr[NUM_VECTOR_UNITS - 1U - i] = static_cast<T>(0);
            }
        }

        if (B_thread_block_tile_row_idx < BLOCK_TILE_SIZE_K && B_thread_block_tile_col_idx < BLOCK_TILE_SIZE_X)
        {
            *reinterpret_cast<int4*>(&B_thread_block_tile[B_thread_block_tile_row_idx][B_thread_block_tile_col_idx]) = B_row_vector_vals;
        }
    }
}

template <typename T, size_t BLOCK_TILE_SIZE_X, size_t BLOCK_TILE_SIZE_Y, size_t BLOCK_TILE_SIZE_K, size_t THREAD_TILE_SIZE_X, size_t THREAD_TILE_SIZE_Y>
__global__ void gemm_2d_block_tiling_2d_thread_tiling_matrix_transpose_vec_mem_access(size_t m, size_t n, size_t k, T alpha, T const* A, size_t lda, T const* B, size_t ldb, T beta, T* C, size_t ldc)
{
    constexpr size_t NUM_THREADS{BLOCK_TILE_SIZE_X * BLOCK_TILE_SIZE_Y / (THREAD_TILE_SIZE_X * THREAD_TILE_SIZE_Y)};
    size_t const thread_linear_idx{threadIdx.y * blockDim.x + threadIdx.x};

    // cache tiles A and B
    __shared__ T A_thread_block_tile_transposed[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_Y];
    __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];

    size_t const num_thread_block_tiles{(k + BLOCK_TILE_SIZE_K - 1) / BLOCK_TILE_SIZE_K};

    T C_thread_results[THREAD_TILE_SIZE_Y][THREAD_TILE_SIZE_X] = {static_cast<T>(0)};

    // A vals and B vals is cached in the regsiter
    T A_vals[THREAD_TILE_SIZE_Y] = {static_cast<T>(0)};
    T B_vals[THREAD_TILE_SIZE_X] = {static_cast<T>(0)};

   constexpr size_t NUM_VECTOR_UNITS{sizeof(int4) / sizeof(T)};
   static_assert(sizeof(int4) % sizeof(T) == 0U);
   static_assert(BLOCK_TILE_SIZE_K % NUM_VECTOR_UNITS == 0U);
   static_assert(BLOCK_TILE_SIZE_X % NUM_VECTOR_UNITS == 0U);
   constexpr size_t VECTORIZED_BLOCK_TILE_SIZE_X{THREAD_TILE_SIZE_X / NUM_VECTOR_UNITS};
   static_assert(THREAD_TILE_SIZE_X % NUM_VECTOR_UNITS == 0U);

   for (size_t thread_block_tile_idx{0U}; thread_block_tile_idx < num_thread_block_tiles; ++thread_block_tile_idx)
   {
        load_data_from_global_memory_to_shared_memory_transposed_vectorized<T, BLOCK_TILE_SIZE_X, BLOCK_TILE_SIZE_Y, BLOCK_TILE_SIZE_K, NUM_THREADS>(A, lda, B, ldb, A_thread_block_tile_transposed, B_thread_block_tile, thread_block_tile_idx, thread_linear_idx, m, n, k);
        __syncthreads();

        #pragma unroll
        for (size_t k_i{0U}; k_i < BLOCK_TILE_SIZE_K; ++k_i)
        {
            size_t const A_thread_block_tile_row_idx{thread_linear_idx / (BLOCK_TILE_SIZE_X / THREAD_TILE_SIZE_X) * THREAD_TILE_SIZE_Y};
            size_t const A_thread_block_tile_col_idx{k_i};

            #pragma unroll
            for (size_t thread_tile_row_idx{0U}; thread_tile_row_idx < THREAD_TILE_SIZE_Y; ++thread_tile_row_idx)
            {
                A_vals[thread_tile_row_idx] = A_thread_block_tile_transposed[A_thread_block_tile_col_idx][A_thread_block_tile_row_idx + thread_tile_row_idx];
            }

            size_t const B_thread_block_tile_row_idx{k_i};
            size_t const B_thread_block_tile_col_idx{thread_linear_idx % (BLOCK_TILE_SIZE_X / THREAD_TILE_SIZE_X) * THREAD_TILE_SIZE_X};

            #pragma unroll
            for (size_t thread_tile_col_vector_idx{0U}; thread_tile_col_vector_idx < VECTORIZED_BLOCK_TILE_SIZE_X; ++thread_tile_col_vector_idx)
            {
                *reinterpret_cast<int4*>(&B_vals[thread_tile_col_vector_idx * NUM_VECTOR_UNITS]) = *reinterpret_cast<int4 const*>(&B_thread_block_tile[B_thread_block_tile_row_idx][B_thread_block_tile_col_idx + thread_tile_col_vector_idx * NUM_VECTOR_UNITS]);
            }

            for (size_t thread_tile_row_idx{0U}; thread_tile_row_idx < THREAD_TILE_SIZE_Y; ++thread_tile_row_idx)
            {
                for (size_t thread_tile_col_idx{0U}; thread_tile_col_idx < THREAD_TILE_SIZE_X; ++thread_tile_col_idx)
                {
                    C_thread_results[thread_tile_row_idx][thread_tile_col_idx] += A_vals[thread_tile_row_idx] * B_vals[thread_tile_col_idx];
                }
            }
        }
        __syncthreads();
   }

   // vectorized writing to DRAM
   for (size_t thread_tile_row_idx{0U}; thread_tile_row_idx < THREAD_TILE_SIZE_Y; ++thread_tile_row_idx)
   {
        for (size_t thread_tile_col_vector_idx{0U}; thread_tile_col_vector_idx < VECTORIZED_BLOCK_TILE_SIZE_X; ++thread_tile_col_vector_idx)
        {
            size_t const C_row_idx{blockIdx.y * BLOCK_TILE_SIZE_Y + thread_linear_idx / (BLOCK_TILE_SIZE_X / THREAD_TILE_SIZE_X) * THREAD_TILE_SIZE_Y + thread_tile_row_idx};
            size_t const C_col_idx{blockIdx.x * BLOCK_TILE_SIZE_X + thread_linear_idx % (BLOCK_TILE_SIZE_X / THREAD_TILE_SIZE_X) * THREAD_TILE_SIZE_X + thread_tile_col_vector_idx + NUM_VECTOR_UNITS};

            // vectorized read from C
            int4 C_row_vector_vals{*reinterpret_cast<int4 const*>(&C[C_row_idx * ldc + C_col_idx])};

            // vectorized read from C_thread_results
            int4 const C_thread_results_row_vector_vals{*reinterpret_cast<int4 const*>(&C_thread_results[thread_tile_row_idx][thread_tile_col_vector_idx * NUM_VECTOR_UNITS])};

            for (size_t i{0U}; i < NUM_VECTOR_UNITS; ++i)
            {
                reinterpret_cast<T*>(&C_row_vector_vals)[i] = alpha * reinterpret_cast<T const*>(&C_thread_results_row_vector_vals)[i] + beta * reinterpret_cast<T const*>(&C_row_vector_vals)[i];
            }

            // vectorized write to C
            if (C_row_idx < m && C_col_idx < n)
            {
                *reinterpret_cast<int4*>(&C[C_row_idx * ldc + C_col_idx]) = C_row_vector_vals;
            }
        }
   }
}

template <typename T>
void launch_gemm_2d_block_tiling_2d_thread_tiling_matrix_transpose_vec_mem_access(size_t m, size_t n, size_t k, T const* alpha, T const* A, size_t lda, T const* B, size_t ldb, T const* beta, T* C, size_t ldc, cudaStream_t stream)
{
    // Feel free to play with the block tile sizes.
    // The algorithm correctness should always be guaranteed.
    constexpr unsigned int BLOCK_TILE_SIZE_X{128U};
    constexpr unsigned int BLOCK_TILE_SIZE_Y{128U};
    constexpr unsigned int BLOCK_TILE_SIZE_K{16U};
    // Each thread computes THREAD_TILE_SIZE_X * THREAD_TILE_SIZE_Y values of C.
    constexpr unsigned int THREAD_TILE_SIZE_X{8U};
    constexpr unsigned int THREAD_TILE_SIZE_Y{8U};
    constexpr unsigned int NUM_THREADS_PER_BLOCK{
        BLOCK_TILE_SIZE_X * BLOCK_TILE_SIZE_Y /
        (THREAD_TILE_SIZE_X * THREAD_TILE_SIZE_Y)};
    static_assert(BLOCK_TILE_SIZE_X % THREAD_TILE_SIZE_X == 0U);
    static_assert(BLOCK_TILE_SIZE_Y % THREAD_TILE_SIZE_Y == 0U);
    static_assert(NUM_THREADS_PER_BLOCK % BLOCK_TILE_SIZE_K == 0U);
    static_assert(NUM_THREADS_PER_BLOCK % BLOCK_TILE_SIZE_X == 0U);
    static_assert(
        BLOCK_TILE_SIZE_X * BLOCK_TILE_SIZE_K % NUM_THREADS_PER_BLOCK == 0U);
    static_assert(
        BLOCK_TILE_SIZE_K * BLOCK_TILE_SIZE_Y % NUM_THREADS_PER_BLOCK == 0U);
    dim3 const block_dim{NUM_THREADS_PER_BLOCK, 1U, 1U};
    dim3 const grid_dim{
        (static_cast<unsigned int>(n) + BLOCK_TILE_SIZE_X - 1U) /
            BLOCK_TILE_SIZE_X,
        (static_cast<unsigned int>(m) + BLOCK_TILE_SIZE_Y - 1U) /
            BLOCK_TILE_SIZE_Y,
        1U};
    gemm_v05_vectorized<T, BLOCK_TILE_SIZE_X, BLOCK_TILE_SIZE_Y,
                        BLOCK_TILE_SIZE_K, THREAD_TILE_SIZE_X,
                        THREAD_TILE_SIZE_Y>
        <<<grid_dim, block_dim, 0U, stream>>>(m, n, k, *alpha, A, lda, B, ldb,
                                                *beta, C, ldc);
    CHECK_LAST_CUDA_ERROR();
}

// Explicit instantiation.
template void launch_gemm_2d_block_tiling_2d_thread_tiling_matrix_transpose_vec_mem_access<float>(
    size_t m, size_t n, size_t k, float const* alpha, float const* A,
    size_t lda, float const* B, size_t ldb, float const* beta, float* C,
    size_t ldc, cudaStream_t stream);
template void launch_gemm_2d_block_tiling_2d_thread_tiling_matrix_transpose_vec_mem_access<double>(
    size_t m, size_t n, size_t k, double const* alpha, double const* A,
    size_t lda, double const* B, size_t ldb, double const* beta, double* C,
    size_t ldc, cudaStream_t stream);
template void launch_gemm_2d_block_tiling_2d_thread_tiling_matrix_transpose_vec_mem_access<__half>(
    size_t m, size_t n, size_t k, __half const* alpha, __half const* A,
    size_t lda, __half const* B, size_t ldb, __half const* beta, __half* C,
    size_t ldc, cudaStream_t stream);